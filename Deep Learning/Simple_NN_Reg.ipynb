{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Depth-1 Neural Network for Classical Linear Regression\n",
    "\n",
    "The classical linear regression admits an observed matrix of random variable $X$ and the corresponding vector of random variable $Y$. The model assumes that $\\vec{x_i}$ for $i = 1...p$ in $X\\in R^{n\\times p}$ are iid. normally distributed, and the conditional mean of $Y$ depends on $X$ has a linear relationship with some parameters. Namely,\n",
    "\n",
    "$$\n",
    "    E[Y|X=x] = \\hat{Y} = Xw + b\n",
    "$$\n",
    "\n",
    "where $w$ and $b$ are unknown."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function\n",
    "\n",
    "The common loss function for linear regression is squared $L^2$ Norm which quantifies the distance between two surfaces. \n",
    "$$\n",
    "L(w,b) = \\sum_{i\\in[n]}(\\hat{y_i}-y_i)^2 = ||\\hat{Y} - Y||^2_2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, unlike most models, linear regression has an analytical solutions:\n",
    "$$\n",
    "w^* = (X^TX)^{-1}X^TY\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minibatch Optimization\n",
    "Both GD and SGD algorithm have notable drawbacks in practial application. The intermediate approach, minibatch SGD, is often used. Let $B$ be a fixed number of training samples, define the update rule to be:\n",
    "$$\n",
    "w_{t+1} = w_t - \\frac{\\eta}{|B_t|}\\sum_{i\\in B_t}\\partial_{w_t}l^{(i)}(w,b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remark: Based on the normality assumption of the data, we can deduce that the Maximum Likelihood Method is equivalent to Minimizing the Squared Loss if we omit the nuissance parameter $\\sigma^2$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthesize Toy Data\n",
    "\n",
    "Recall that the actual model in simple linear regression is \n",
    "$$\n",
    "Y = Xw + b + \\epsilon\n",
    "$$\n",
    "where $\\epsilon \\sim N(0, \\sigma)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import dl\n",
    "import torch as tor\n",
    "import random\n",
    "from typing import Generator\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "class ToyRegressionData(dl.DataModule):\n",
    "    def __init__(self, w, b, eps_var=1, num_train=1000, num_val=100, batch_size=32):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        n = num_train + num_val\n",
    "        self.X = tor.randn(n, len(w))\n",
    "        noise = tor.normal(\n",
    "            mean = dl.make_vec_single(0, n),\n",
    "            std = dl.make_vec_single(b, n)\n",
    "        )\n",
    "        self.y = tor.matmul(self.X, w.reshape(-1, 1)) + b + noise.reshape(-1, 1)\n",
    "    \n",
    "    #override\n",
    "    def get_dataloader(self, train=True):\n",
    "        if train:\n",
    "            indices = list(range(0, self.num_train))\n",
    "            random.shuffle(indices)\n",
    "        else:\n",
    "            indices = list(range(self.num_train, self.num_train+self.num_val))\n",
    "        \n",
    "        for i in range(0, len(indices), self.batch_size):\n",
    "            batch_indices = tor.tensor(indices[i: i + self.batch_size])\n",
    "            yield self.X[batch_indices], self.y[batch_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3])\n",
      "torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "\n",
    "d = ToyRegressionData(w=tor.tensor([1,2,3], dtype=tor.float32), b=4)\n",
    "[print(item.shape) for item in next(d.get_dataloader())];"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition\n",
    "Any pyroch deep learning model should be a subclass of `Module` class. Tree basic definition should be implemented\\overwritten by the actual model:\n",
    "- `__init__` method initializes the weight appropirately\n",
    "- `forward` method defines the network architecture\n",
    "- `loss` method computes the current loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(dl.Module):\n",
    "    def __init__(self, P, eta, sigma=0.01):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.w = tor.normal(0, sigma, (P, 1), requires_grad=True)\n",
    "        self.b = tor.zeros(1, requires_grad=True)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return tor.matmul(X, self.w) + self.b\n",
    "    \n",
    "    def loss(self, y_hat, y):\n",
    "        l = (y_hat - y) ** 2\n",
    "        return l.mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer Definition\n",
    "An optimizer is the implementation of the optimization algorithm. It is a subclass of `HyperParameter` class. The `configure_optimizer` method in `Module` class adjust the optimizer appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(dl.HyperPrameters):\n",
    "    def __init__(self, w, b, y_hat, eta):\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def step(self):\n",
    "        w -= self.eta * 2 * self.X.T @ (self.X @ w + b - self.y_hat)\n",
    "        b -= self.eat * 2 * (self.X @ w + b - self.y_hat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
